{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Car2DEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 2\n",
    "    }\n",
    "     \n",
    "    def __init__(self):\n",
    "        self.xth = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.L = 10\n",
    "        self.action_space = spaces.Discrete(5) # # 0:stay, 1:up, 2:down，3:left，4:right\n",
    "        self.observation_space = spaces.Box(np.array([-self.L, -self.L]), np.array([self.L, self.L]))\n",
    "        self.state = None\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        x, y = self.state\n",
    "        if action == 0:\n",
    "            x = x\n",
    "            y = y\n",
    "        if action == 1:\n",
    "            x = x\n",
    "            y = y + 1\n",
    "        if action == 2:\n",
    "            x = x\n",
    "            y = y - 1\n",
    "        if action == 3:\n",
    "            x = x - 1\n",
    "            y = y\n",
    "        if action == 4:\n",
    "            x = x + 1\n",
    "            y = y\n",
    "        self.state = np.array([x, y])\n",
    "        self.counts += 1\n",
    "        \n",
    "        done = (np.abs(x)+np.abs(y) <= 1) or (np.abs(x)+np.abs(y) >= 2*self.L+1)\n",
    "        done = bool(done)\n",
    "        \n",
    "        if not done:\n",
    "            reward = -0.1\n",
    "        else:\n",
    "            if np.abs(x)+np.abs(y) <= 1:\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -50\n",
    "            \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.ceil(np.random.rand(2)*2*self.L)-self.L\n",
    "        self.counts = 0\n",
    "        return self.state\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return None\n",
    "        \n",
    "    def close(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Car2DEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/renyong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 677\n",
      "Trainable params: 677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renyong/anaconda3/lib/python3.6/site-packages/keras_rl-0.4.2-py3.6.egg/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84/1000: episode: 1, duration: 0.467s, episode steps: 84, steps per second: 180, episode reward: -58.300, mean reward: -0.694 [-50.000, -0.100], mean action: 1.631 [0.000, 4.000], mean observation: -6.619 [-16.000, -2.000], loss: 0.089281, mean_absolute_error: 0.529797, mean_q: 1.022866\n",
      " 188/1000: episode: 2, duration: 0.206s, episode steps: 104, steps per second: 505, episode reward: -60.300, mean reward: -0.580 [-50.000, -0.100], mean action: 1.471 [0.000, 4.000], mean observation: -1.611 [-13.000, 10.000], loss: 8.040032, mean_absolute_error: 0.989454, mean_q: 1.360462\n",
      " 214/1000: episode: 3, duration: 0.054s, episode steps: 26, steps per second: 481, episode reward: -52.500, mean reward: -2.019 [-50.000, -0.100], mean action: 1.346 [0.000, 4.000], mean observation: 8.038 [4.000, 12.000], loss: 9.882359, mean_absolute_error: 1.315702, mean_q: 1.625490\n",
      " 324/1000: episode: 4, duration: 0.216s, episode steps: 110, steps per second: 510, episode reward: -60.900, mean reward: -0.554 [-50.000, -0.100], mean action: 1.527 [0.000, 4.000], mean observation: -1.823 [-17.000, 6.000], loss: 15.152317, mean_absolute_error: 1.549525, mean_q: 1.734102\n",
      " 339/1000: episode: 5, duration: 0.030s, episode steps: 15, steps per second: 494, episode reward: -51.400, mean reward: -3.427 [-50.000, -0.100], mean action: 2.133 [0.000, 4.000], mean observation: 8.333 [4.000, 11.000], loss: 13.535320, mean_absolute_error: 1.604620, mean_q: 1.860576\n",
      " 407/1000: episode: 6, duration: 0.131s, episode steps: 68, steps per second: 518, episode reward: 3.300, mean reward: 0.049 [-0.100, 10.000], mean action: 1.279 [0.000, 4.000], mean observation: -1.544 [-10.000, 4.000], loss: 17.008999, mean_absolute_error: 1.503047, mean_q: 1.745592\n",
      " 419/1000: episode: 7, duration: 0.024s, episode steps: 12, steps per second: 491, episode reward: 8.900, mean reward: 0.742 [-0.100, 10.000], mean action: 1.417 [0.000, 3.000], mean observation: -0.042 [-5.000, 4.000], loss: 10.271768, mean_absolute_error: 1.420130, mean_q: 1.746564\n",
      " 477/1000: episode: 8, duration: 0.116s, episode steps: 58, steps per second: 500, episode reward: -55.700, mean reward: -0.960 [-50.000, -0.100], mean action: 1.103 [0.000, 3.000], mean observation: -6.000 [-20.000, 1.000], loss: 11.126669, mean_absolute_error: 1.402781, mean_q: 1.811868\n",
      " 501/1000: episode: 9, duration: 0.047s, episode steps: 24, steps per second: 509, episode reward: -52.300, mean reward: -2.179 [-50.000, -0.100], mean action: 1.958 [0.000, 4.000], mean observation: -1.833 [-15.000, 6.000], loss: 9.733211, mean_absolute_error: 1.358730, mean_q: 1.993296\n",
      " 622/1000: episode: 10, duration: 0.237s, episode steps: 121, steps per second: 511, episode reward: -2.000, mean reward: -0.017 [-0.100, 10.000], mean action: 1.298 [0.000, 4.000], mean observation: -1.450 [-11.000, 9.000], loss: 17.377357, mean_absolute_error: 1.520336, mean_q: 2.019513\n",
      " 749/1000: episode: 11, duration: 0.245s, episode steps: 127, steps per second: 518, episode reward: -62.600, mean reward: -0.493 [-50.000, -0.100], mean action: 1.787 [0.000, 4.000], mean observation: -2.020 [-12.000, 13.000], loss: 13.843125, mean_absolute_error: 1.590646, mean_q: 2.111978\n",
      " 808/1000: episode: 12, duration: 0.113s, episode steps: 59, steps per second: 521, episode reward: 4.200, mean reward: 0.071 [-0.100, 10.000], mean action: 1.508 [0.000, 4.000], mean observation: -1.085 [-7.000, 3.000], loss: 17.106314, mean_absolute_error: 1.740165, mean_q: 2.331187\n",
      " 916/1000: episode: 13, duration: 0.209s, episode steps: 108, steps per second: 517, episode reward: -0.700, mean reward: -0.006 [-0.100, 10.000], mean action: 1.093 [0.000, 4.000], mean observation: -1.986 [-11.000, 4.000], loss: 10.594994, mean_absolute_error: 1.714801, mean_q: 2.375957\n",
      "done, took 2.261 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb291468d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rl.agents.dqn.DQNAgent"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 4.]\n"
     ]
    }
   ],
   "source": [
    "obs, done = env.reset(), False\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [2. 3.]\n",
      "state: [3. 3.]\n",
      "state: [3. 3.]\n",
      "state: [3. 3.]\n",
      "state: [3. 2.]\n",
      "state: [3. 1.]\n",
      "state: [3. 1.]\n",
      "state: [2. 1.]\n",
      "state: [2. 1.]\n",
      "state: [2. 0.]\n",
      "state: [2. 1.]\n",
      "state: [1. 1.]\n",
      "state: [1. 0.]\n",
      "[8.8, 13]\n"
     ]
    }
   ],
   "source": [
    "episode_reward = 0\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(dqn.forward(obs))\n",
    "    print(\"state:\",obs)\n",
    "    episode_reward += reward\n",
    "print([episode_reward, env.counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
